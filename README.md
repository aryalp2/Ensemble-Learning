# Ensemble-Learning
These are techniques and explore different methods for constructing a set of classifiers that can
often have a better predictive performance than any of its individual members. We will do the following:
Make predictions based on majority voting,
Use bagging to reduce overfitting by drawing random combinations of the training set with repetition, Apply boosting to build powerful models from weak learners that learn from
their mistakes
